<html class="linux chrome" dir="ltr" data-bs-theme="light">
    <head>
    <meta charset="utf-8">
    <title>card layout</title>
<!-- <base href="http://127.0.0.1:45901/"><link rel="stylesheet" type="text/css" href="http://127.0.0.1:45901/_anki/css/webview.css"> -->
<script type="module" src="/src/main.jsx"></script>
</head>

<body class="card card1 isLin fancy">
    
<script src="http://127.0.0.1:45901/_anki/js/webview.js"></script>
<script src="http://127.0.0.1:45901/_anki/js/mathjax.js"></script>
<script src="http://127.0.0.1:45901/_anki/js/vendor/mathjax/tex-chtml-full.js"></script>
<script src="http://127.0.0.1:45901/_anki/js/reviewer.js"></script>

<div id="_mark" hidden="">★</div>
<div id="_flag" hidden="">⚑</div>

<div id="front-card-cloze">
# A Dummy Card Subtitle<br><br>```php<br><br>$x = "teste";<br>$a = 123<br>```
<br>&nbsp;```php
<br>$x =  "teste";
<br>```<br><br>### Math<br>- <span class="cloze" data-cloze="$\pi=3.1415...$" data-ordinal="1">[$\pi=...$]</span><br>### Escaping dollar<br><br>$$1\$ &lt; 1€$$ <br>after<br><br>### Escaped c++ code :<br><br>```cpp<br>#include &lt;stdio.h&gt;<br><span class="cloze-inactive" data-ordinal="2">boost:\:add_vertex</span>(...);<br>```<br><br>### Python<br><br>```py<br>def print_hello():<br>&nbsp; print("<span class="cloze-inactive" data-ordinal="2">hello</span>")<br>```
</div>

<div id="back-card-cloze" class="field">

$123$

The <span class="cloze" data-ordinal="1">"Tokens/sec"</span> metric demonstrated significant training speed advantages of FlashAttentionV3 across various `ATTENTION_TYPES` and `SEQLENS`.<br><br><br>\$123 is greater than \%123 or 88\$.<br><br>\$123 is greater than \%123 or 88\$.<br><br><br>```python<br>print("Hello <span class="cloze" data-ordinal="1">World</span>")<br>```
</div>
<div id="extra-card-cloze" class="field">
This indicates that even with optimized attention mechanisms, there are still memory limitations that can be hit at very long sequence lengths, leading to a sharp decline in performance.
</div>

<div id="tags-card" class="field">
    Computer_Science::Benchmarking Computer_Science::GPU_Computing Deep_Learning::Performance_Metrics
</div>

<div id="difficulty-card" class="field">
    2/10
</div>


<div id="front-card-basic">
# Hello<br><br>## How are you?
</div>

<div id="back-card-basic">
Both PCA and k-means clustering are solving the same fundamental problem of **compressing data with maximum fidelity** (minimizing reconstruction error), but under **different constraints** on the model complexity. They can be unified under a broader class of techniques known as **matrix factorization methods**.
</div>
<div id="extra-card-basic">
In PCA, the matrix $W$ (principal components) must be orthogonal, while in k-means, $W$ (cluster centroids) is arbitrary, and $Z$ (cluster assignments) encodes which cluster each point belongs to.
</div>


</div>
</body></html>


